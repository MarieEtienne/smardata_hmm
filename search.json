[
  {
    "objectID": "lab_hmm.html",
    "href": "lab_hmm.html",
    "title": "Analyzing Masked Booby Movement Using HMM",
    "section": "",
    "text": "$$\n$$"
  },
  {
    "objectID": "lab_hmm.html#data-presentation",
    "href": "lab_hmm.html#data-presentation",
    "title": "Analyzing Masked Booby Movement Using HMM",
    "section": "Data Presentation",
    "text": "Data Presentation\nWe focus on studying the behavior of Red-footed Boobies (Sula sula).\nOur dataset consists of three trajectories recorded for three Red-footed Boobies from Ilha do Meio, a Brazilian island part of the Fernando de Noronha archipelago in the Atlantic Ocean."
  },
  {
    "objectID": "lab_hmm.html#projection",
    "href": "lab_hmm.html#projection",
    "title": "Analyzing Masked Booby Movement Using HMM",
    "section": "Projection",
    "text": "Projection\nThe data for this booby are specified in latitude and longitude, which means they are based on angular coordinates on Earth’s surface. Methods described earlier rely on distance metrics (e.g., step lengths). While it’s possible to compute traveled distances using latitude and longitude, this involves formulas specific to spherical movements.\nTo circumvent this and use Euclidean distance, data are often projected into a local system. This projection must match the region of interest. Here, we use UTM coordinates for Zone 25 South. Geographic data manipulation is made easier in R using the sf package, which we use to project the data for easier distance calculations."
  },
  {
    "objectID": "lab_hmm.html#defining-metrics",
    "href": "lab_hmm.html#defining-metrics",
    "title": "Analyzing Masked Booby Movement Using HMM",
    "section": "Defining Metrics",
    "text": "Defining Metrics\nWe aim to identify different behavioral types within the trajectory, such as foraging (associated with rapid direction changes) and goal-directed travel (characterized by straighter trajectories). Relevant metrics here are turning angle and step length, as these are biologically meaningful for these birds.\nWe fit two models differing by the considered movement metrics (and their corresponding emission distributions): - Step Length/Angle: Metrics from Morales et al. (2004), using gamma distribution for step length and von Mises distribution for angles. - Bivariate Velocity: Metrics from Gurarie, Andrews, and Laidre (2009), using two independent normal distributions."
  },
  {
    "objectID": "lab_hmm.html#initializing-the-algorithm",
    "href": "lab_hmm.html#initializing-the-algorithm",
    "title": "Analyzing Masked Booby Movement Using HMM",
    "section": "Initializing the Algorithm",
    "text": "Initializing the Algorithm\nThese models without covariates have 18 parameters (12 for emission distributions and 6 for transition probabilities). Optimization in such a parameter space is sensitive to initial values. We use a simple k-means classification to find plausible initial parameters for the regimes."
  },
  {
    "objectID": "lab_hmm.html#characterizing-hidden-states",
    "href": "lab_hmm.html#characterizing-hidden-states",
    "title": "Analyzing Masked Booby Movement Using HMM",
    "section": "Characterizing Hidden States",
    "text": "Characterizing Hidden States\nFor both packages used, HMM parameters are estimated via maximum likelihood, and the most probable sequence of hidden states is retrieved using the Viterbi algorithm. Hidden states correspond to velocity and turning angle distributions. In trajectory terms, they describe segments between two positions (separated by 10 seconds)."
  },
  {
    "objectID": "hmm_lecture.html#movement-ecology-paradigm",
    "href": "hmm_lecture.html#movement-ecology-paradigm",
    "title": "Introduction to Hidden markov Model",
    "section": "Movement Ecology Paradigm",
    "text": "Movement Ecology Paradigm\n[Nat+08] presents individual movement as the result of:\n\n\n\n\n\nMovement drivers by Nathan, Getz, Revilla et al. [Nat+08].\n\n\n\n\nMotion capacities\nInternal state\nEnvironment\n\n\nMovement informs on internal states and habitat preferences"
  },
  {
    "objectID": "hmm_lecture.html#from-movement-to-movement-data",
    "href": "hmm_lecture.html#from-movement-to-movement-data",
    "title": "Introduction to Hidden markov Model",
    "section": "From Movement to Movement Data",
    "text": "From Movement to Movement Data"
  },
  {
    "objectID": "hmm_lecture.html#from-movement-to-movement-data-1",
    "href": "hmm_lecture.html#from-movement-to-movement-data-1",
    "title": "Introduction to Hidden markov Model",
    "section": "From Movement to Movement Data",
    "text": "From Movement to Movement Data"
  },
  {
    "objectID": "hmm_lecture.html#from-movement-to-movement-data-2",
    "href": "hmm_lecture.html#from-movement-to-movement-data-2",
    "title": "Introduction to Hidden markov Model",
    "section": "From Movement to Movement Data",
    "text": "From Movement to Movement Data"
  },
  {
    "objectID": "hmm_lecture.html#from-movement-to-movement-data-3",
    "href": "hmm_lecture.html#from-movement-to-movement-data-3",
    "title": "Introduction to Hidden markov Model",
    "section": "From Movement to Movement Data",
    "text": "From Movement to Movement Data"
  },
  {
    "objectID": "hmm_lecture.html#description",
    "href": "hmm_lecture.html#description",
    "title": "Introduction to Hidden markov Model",
    "section": "Description",
    "text": "Description"
  },
  {
    "objectID": "hmm_lecture.html#continuous-process-and-sampling",
    "href": "hmm_lecture.html#continuous-process-and-sampling",
    "title": "Introduction to Hidden markov Model",
    "section": "Continuous Process and Sampling",
    "text": "Continuous Process and Sampling\n\n\nA continuous process sampled at some discrete, potentially irregular times.\nTime series with values in \\(\\mathbb{R}^2\\) (on Earth…).\n\\[\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{Time} & \\text{Location} & \\text{Turning Angle} & \\text{Speed} \\\\\n\\hline\nt_{0} & (x_0, y_0) & \\text{NA} & \\text{NA} \\\\\nt_{1} & (x_1, y_1) & \\text{NA} & sp_1 \\\\\nt_{2} & (x_2, y_2) & ang_2 & sp_2 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nt_{n} & (x_n, y_n) & ang_n & sp_n \\\\\n\\hline\n\\end{array}\n\\]"
  },
  {
    "objectID": "hmm_lecture.html#ecological-question-identifying-various-movement-patterns",
    "href": "hmm_lecture.html#ecological-question-identifying-various-movement-patterns",
    "title": "Introduction to Hidden markov Model",
    "section": "Ecological question: Identifying various movement patterns",
    "text": "Ecological question: Identifying various movement patterns\n\n\nRarely in a supervised context\n\n\n\nArtisanal fishing trips in Toliara, Madagascar\n\n\n\nMostly unsupervised\n\n\n\nPeruvian booby data courtesy of Sophie Bertrand."
  },
  {
    "objectID": "hmm_lecture.html#from-movement-data-to-movement-model",
    "href": "hmm_lecture.html#from-movement-data-to-movement-model",
    "title": "Introduction to Hidden markov Model",
    "section": "From Movement Data to Movement Model",
    "text": "From Movement Data to Movement Model\nOften analysed using discrete time model and typically Hidden Markov Model\n\n\n\nMovement decomposition"
  },
  {
    "objectID": "hmm_lecture.html#heterogeneity-in-movement-pattern-interpretated-as-different-internal-states",
    "href": "hmm_lecture.html#heterogeneity-in-movement-pattern-interpretated-as-different-internal-states",
    "title": "Introduction to Hidden markov Model",
    "section": "Heterogeneity in movement pattern interpretated as different internal states",
    "text": "Heterogeneity in movement pattern interpretated as different internal states\n\n\n\n\n\nPeruvian booby data courtesy of Sophie Bertrand\n\n\n\n\n\nMovement decomposition"
  },
  {
    "objectID": "hmm_lecture.html#heterogeneity-in-movement-pattern-interpretated-as-different-internal-states-1",
    "href": "hmm_lecture.html#heterogeneity-in-movement-pattern-interpretated-as-different-internal-states-1",
    "title": "Introduction to Hidden markov Model",
    "section": "Heterogeneity in movement pattern interpretated as different internal states",
    "text": "Heterogeneity in movement pattern interpretated as different internal states\n\n\n\n\n\nPeruvian booby data courtesy of Sophie Bertrand\n\n\n\n\n\nMovement decomposition"
  },
  {
    "objectID": "hmm_lecture.html#accounting-for-internal-states",
    "href": "hmm_lecture.html#accounting-for-internal-states",
    "title": "Introduction to Hidden markov Model",
    "section": "Accounting for internal states",
    "text": "Accounting for internal states\n\n\n\nChange point detection approach. based on [Pic+07]\n\n\n\n\nSegmentation illustration\n\n\n\\[\\left .\\begin{array}{c}\nY_{1k} \\sim f(\\theta_{1\\ell})\\\\\nY_{2k} \\sim f(\\theta_{2\\ell})\n\\end{array} \\right\\rbrace \\mbox{ if } k \\mbox{ in region }I_{\\ell}=[\\tau_{\\ell-1}+1,\\tau_{\\ell}]\\]\n\nThe best segmentation for a given number of change points L: \\[argmin_{0&lt; \\tau_1 &lt; \\ldots &lt; \\tau_L} C(\\boldsymbol{Y}) \\]\nBrute force complexity : \\(O(N^L)\\),\nDynamic Programming algorithm complexity : \\(O(N^2)\\) or even less with pruning strategy\nLimitation in the choice for \\(f.\\)"
  },
  {
    "objectID": "hmm_lecture.html#a-simple-latent-variables-models",
    "href": "hmm_lecture.html#a-simple-latent-variables-models",
    "title": "Introduction to Hidden markov Model",
    "section": "A simple Latent variables models",
    "text": "A simple Latent variables models\n\nA parametric model (\\(M, \\boldsymbol{\\theta}\\)) produces \\({\\boldsymbol{Y}}\\) and \\({\\boldsymbol{Z}}\\).\nThe only observed data are \\({\\boldsymbol{Y}}\\) while \\({\\boldsymbol{Z}}\\) are hidden variables.\n\nQuestions are\n\nParameters: Is it still possible to estimate \\(\\boldsymbol{\\theta}\\) ?\nInformation on \\({\\boldsymbol{Z}}\\): is it possible to “reconstruct” the unobserved data \\({\\boldsymbol{Z}}\\) ?\n\nBayes formula is the key :\n\\[{\\mathbb{P}}({\\boldsymbol{Y}}, {\\boldsymbol{Z}})={\\mathbb{P}}({\\boldsymbol{Y}}\\vert {\\boldsymbol{Z}}){\\mathbb{P}}({\\boldsymbol{Z}})={\\mathbb{P}}({\\boldsymbol{Z}}\\vert {\\boldsymbol{Y}}) {\\mathbb{P}}({\\boldsymbol{Y}})\\]"
  },
  {
    "objectID": "hmm_lecture.html#mixture-model-as-a-simple-latent-variables-model",
    "href": "hmm_lecture.html#mixture-model-as-a-simple-latent-variables-model",
    "title": "Introduction to Hidden markov Model",
    "section": "Mixture model as a simple Latent variables model",
    "text": "Mixture model as a simple Latent variables model\n\n\nAssume that the numer of states \\(K\\) is known\n\nModelling \\(Z\\): \\(\\pi_k={\\mathbb{P}}(Z_i=k), \\quad k=1,\\ldots, K, \\quad \\sum_k \\pi_k=1\\)\n\\(Z_i \\overset{i.i.d}{\\sim} \\mathcal{M}(1, \\boldsymbol{\\pi}), \\quad P(Z_{ik}=1)=\\pi_k\\)\nModelling \\(Y\\)}: The \\(Y_i's\\) are assumed to be independent conditionnaly to \\({\\boldsymbol{Z}}\\) : \\((Y_i\\vert Z_i = k) \\overset{i.i.d}{\\sim} f_{\\gamma_k}().\\)\n\n\nModel parameters \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\pi}, \\boldsymbol{\\gamma})\\)"
  },
  {
    "objectID": "hmm_lecture.html#mixture-model-as-a-simple-latent-variables-model-1",
    "href": "hmm_lecture.html#mixture-model-as-a-simple-latent-variables-model-1",
    "title": "Introduction to Hidden markov Model",
    "section": "Mixture model as a simple Latent variables model",
    "text": "Mixture model as a simple Latent variables model\nWe could assume that\n\nA step \\(Y_k\\) is a descriptor of movement between position at time \\(t_{k}\\) and \\(t_{k+1}\\)\nConditionally on some hidden state \\(Z_k = \\ell\\), the distribution of \\(Y_k\\) has some certain parametric distribution, let’s say Gaussian\n\nQuestion: If \\(K=2\\), what are \\(\\boldsymbol{\\pi}\\) and \\(\\boldsymbol{\\gamma}\\) ?"
  },
  {
    "objectID": "hmm_lecture.html#mixture-model-properties",
    "href": "hmm_lecture.html#mixture-model-properties",
    "title": "Introduction to Hidden markov Model",
    "section": "Mixture model Properties",
    "text": "Mixture model Properties\n\nCouples \\(\\{(Y_i, Z_i)\\}\\) are i.i.d.\nLabel switching: the model is invariant for any permutation of the labels \\(\\{1,\n\\dots, K\\}\\) \\(\\Rightarrow\\) the mixture model has [\\(K!\\) equivalent definitions]{.rouge].\nDistribution of a \\(Y_i\\): \\[{\\mathbb{P}}(Y_i)=\\sum_{k=1}^K {\\mathbb{P}}(Y_i, Z_i=k)= \\class{bleu}{{\\mathbb{P}}(Z_i=k)} \\class{orange}{{\\mathbb{P}}(Y_i | Z_i=k)}\\]\nDistribution of \\({\\boldsymbol{Y}}\\):\n\n\\[\\begin{align}\n{\\mathbb{P}}({\\boldsymbol{Y}}; \\boldsymbol{\\theta}) & = \\prod_{i=1}^n \\sum_{k=1}^K P(Y_i , Z_i=k;\\boldsymbol{\\theta})  \\\\\n&= \\prod_{i=1}^n \\sum_{k=1}^K \\class{bleu}{{\\mathbb{P}}(Z_i=k; \\boldsymbol{\\pi})} \\class{orange}{{\\mathbb{P}}(Y_i | Z_i=k; \\boldsymbol{\\gamma})} \\\\\n&= \\prod_{i=1}^n \\sum_{k=1}^K \\class{bleu}{\\pi_k} \\class{orange}{f_{\\gamma_k}(Y_i)}\n\\end{align}\\]"
  },
  {
    "objectID": "hmm_lecture.html#adding-temporal-structure-hidden-markov-model",
    "href": "hmm_lecture.html#adding-temporal-structure-hidden-markov-model",
    "title": "Introduction to Hidden markov Model",
    "section": "Adding temporal structure : Hidden Markov Model",
    "text": "Adding temporal structure : Hidden Markov Model\n\nMarkov chain modelHidden Markov modelExample\n\n\nModelling the dependence in state sequence: If an animal is feeding at time \\(i\\), he has more chance to be feeding at time \\(i+1\\) than if he was travelling at time \\(i\\). \\[{\\mathbb{P}}(Z_{i+1}=1 \\vert Z_{i}=1) \\ne {\\mathbb{P}}(Z_{i+1}=1 \\vert Z_{i}=2)\\]\nMarkov Chain definition\n\\({\\boldsymbol{Z}}\\) is a Markov chain if \\[P(Z_{i+1} \\vert Z_{1:i}) =  P(Z_{i+1} \\vert Z_{i})\\]\nwhere \\(Z_{1:i}\\) stands for \\(Z_{1}, \\ldots, Z_i\\).\n\\({\\boldsymbol{Z}}\\) is completely defined by the initial distribution \\(\\boldsymbol{\\nu}\\), \\(\\nu_k={\\mathbb{P}}(Z_1=k)\\) and the transition matrix \\(\\boldsymbol{\\Pi}\\)\n\n\n\nHidden States \\({\\boldsymbol{Z}}\\) model: \\({\\boldsymbol{Z}}\\) is assumed to follow a Markov Chain model with unknown initial distribution \\(\\boldsymbol{\\nu}\\) and transition matrix \\(\\boldsymbol{\\Pi}\\).\nObservations \\({\\boldsymbol{Y}}\\) model: The \\(Y_i's\\) are assumed to be independent conditionnaly on \\({\\boldsymbol{Z}}\\) : \\((Y_i\\vert Z_i = k) \\overset{i.i.d}{\\sim} f_{\\gamma_k}().\\)\nModel parameters are \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\nu},  \\boldsymbol{\\Pi}, \\boldsymbol{\\gamma})\\)\n\n\n\n\nDefine all the key elements of a hidden Markov model with\n\n\\(K= 3\\) hidden states\nConditionnaly on \\({\\boldsymbol{Z}}\\), \\({\\boldsymbol{Y}}\\) are normally distributed (with \\(K\\) different expeactation and variance)\nSimulate Such a model with your prefered language"
  },
  {
    "objectID": "hmm_lecture.html#statistical-inference-of-incomplete-data-models",
    "href": "hmm_lecture.html#statistical-inference-of-incomplete-data-models",
    "title": "Introduction to Hidden markov Model",
    "section": "Statistical inference of incomplete data models",
    "text": "Statistical inference of incomplete data models\nGoal :\n\\[(\\widehat{\\boldsymbol{\\gamma}},\\widehat{\\boldsymbol{\\Pi}}, \\widehat{\\boldsymbol{\\nu}}) = \\arg\\max_{\\boldsymbol{\\gamma}, \\boldsymbol{\\Pi}, {\\boldsymbol{\\nu}}} \\log P({\\boldsymbol{Y}}; \\boldsymbol{\\gamma}, \\boldsymbol{\\Pi}, \\boldsymbol{\\nu})\\]\n\nObserved likelihoodJoint DistributionEM algorithm\n\n\n\\[\\log {\\mathbb{P}}({\\boldsymbol{Y}}; \\boldsymbol{\\theta})  = \\log  \\sum_{k_1, k_N \\in K^N }{\\mathbb{P}}({\\boldsymbol{Y}}, {\\boldsymbol{Z}}= (k_1, \\ldots,k_N); \\boldsymbol{\\theta})\\] * No analytical estimators.\n\nIt is not always possible to compute since this sum typically involves \\(K^n\\) terms : \\(2^{100}\\approx10^{30}\\)\nBrute force algorithm is not the way\n\n\n\nA convenient notation \\(Z_{ik}=1\\) if \\(Z_i=k\\)\n\\[\\begin{align}\n  \\log {\\mathbb{P}}({\\boldsymbol{Y}}, {\\boldsymbol{Z}}; \\boldsymbol{\\theta}) & =  \\class{bleu}{\\sum_k Z_{1k} \\log \\nu_k }\\\\\n      &\\class{bleu}{+ \\sum_{i &gt; 1} \\sum_{k, \\ell}\n       Z_{i-1,k}Z_{i,\\ell} \\log \\pi_{k\\ell}} \\\\\n      & + \\class{orange}{\\sum_i \\sum_k Z_{ik} \\log f(y_i ; \\gamma_k)}\n\\end{align}\\]\nIdea : replace \\(Z_{ik}\\) by its best guess\n\\[\\begin{align}\n  {\\mathbb{E}}\\left( \\log {\\mathbb{P}}({\\boldsymbol{Y}}, {\\boldsymbol{Z}}; \\boldsymbol{\\theta})\\vert Y_{1:N}\\right)  & =  \\class{bleufonce}{\\sum_k{\\mathbb{E}}\\left( Z_{1k} \\vert Y_{1:N}\\right) \\log \\nu_k }\\\\\n     & \\class{bleufonce}{+ \\sum_{i &gt; 1} \\sum_{k, \\ell}\n      {\\mathbb{E}}\\left( Z_{i-1,k}Z_{i,\\ell}\\vert Y_{1:N}\\right) \\log \\pi_{k\\ell}} \\\\\n     &  + \\class{rouge}{\\sum_i \\sum_k {\\mathbb{E}}\\left(Z_{ik}\\vert Y_{1:N}\\right) \\log f(X_i ; \\gamma_k)}\n\\end{align}\\]\n\n\nExpexctation Maximization (EM) algorithm\n\n\n\n\n\n\nBayes Formula\n\\[\\begin{align}\n{\\mathbb{P}}({\\boldsymbol{Y}}, {\\boldsymbol{Z}};\\boldsymbol{\\theta}) & = {\\mathbb{P}}({\\boldsymbol{Y}}\\vert {\\boldsymbol{Z}}; \\boldsymbol{\\theta}) P({\\boldsymbol{Z}}; \\boldsymbol{\\theta}),\\\\\n& = {\\mathbb{P}}({\\boldsymbol{Z}}\\vert {\\boldsymbol{Y}}; \\boldsymbol{\\theta}) {\\mathbb{P}}({\\boldsymbol{Y}}; \\boldsymbol{\\theta}).\n\\end{align}\\]\nTherefore, \\[\\begin{align}\n\\log {\\mathbb{P}}({\\boldsymbol{Y}}; \\boldsymbol{\\theta}) & = \\log \\left \\lbrace {\\mathbb{P}}({\\boldsymbol{Y}}, {\\boldsymbol{Z}};\\boldsymbol{\\theta}) / {\\mathbb{P}}({\\boldsymbol{Z}}\\vert {\\boldsymbol{Y}}; \\boldsymbol{\\theta}) \\right\\rbrace\\\\\n& = \\log {\\mathbb{P}}({\\boldsymbol{Y}}, {\\boldsymbol{Z}};\\boldsymbol{\\theta}) - \\log {\\mathbb{P}}({\\boldsymbol{Z}}\\vert {\\boldsymbol{Y}}; \\boldsymbol{\\theta}) \\\\\n\\end{align}\\]\nFor a given \\(\\boldsymbol{\\theta}_0\\), we may compute \\({\\mathbb{P}}_{\\boldsymbol{\\theta}_0}=P({\\boldsymbol{Z}}\\vert \\boldsymbol{\\theta}_0, {\\boldsymbol{Y}})\\) and \\[\\begin{align}\n\\log {\\mathbb{P}}({\\boldsymbol{Y}}; \\boldsymbol{\\theta}) &= \\class{orange}{{\\mathbb{E}}_{\\boldsymbol{\\theta}_0}(\\log P({\\boldsymbol{Y}}, {\\boldsymbol{Z}};\\boldsymbol{\\theta})\\vert {\\boldsymbol{Y}})} - {\\mathbb{E}}_{\\boldsymbol{\\theta}_0}(\\log {\\mathbb{P}}({\\boldsymbol{Z}}\\vert {\\boldsymbol{Y}}; \\boldsymbol{\\theta})\\vert {\\boldsymbol{Y}})\\\\\n  & = \\class{orange}{Q(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_0)} - H(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_0)\n  \\end{align}\\]"
  },
  {
    "objectID": "hmm_lecture.html#expectation---maximization-algorithm",
    "href": "hmm_lecture.html#expectation---maximization-algorithm",
    "title": "Introduction to Hidden markov Model",
    "section": "Expectation - Maximization algorithm",
    "text": "Expectation - Maximization algorithm\n\nPhase E : Calculate \\[Q(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{k})\\] for every \\(\\boldsymbol{\\theta}\\).\nPhase M : Define\n\\[\\boldsymbol{\\theta}^{k+1}=argmax\\, Q(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{k})\\]"
  },
  {
    "objectID": "hmm_lecture.html#example-em-for-gaussian-hmm",
    "href": "hmm_lecture.html#example-em-for-gaussian-hmm",
    "title": "Introduction to Hidden markov Model",
    "section": "Example : EM for Gaussian HMM",
    "text": "Example : EM for Gaussian HMM\nLet’s compute it together"
  },
  {
    "objectID": "hmm_lecture.html#exercise",
    "href": "hmm_lecture.html#exercise",
    "title": "Introduction to Hidden markov Model",
    "section": "Exercise",
    "text": "Exercise\n\nSimulate the following HMM\n\nK=2, \\(\\pi_11=0.9\\), \\(pi_{22}=0.8\\)\n\\[Y_i \\vert Z_i = 1 \\sim\\mathcal{N}(2, 1)\\]\n\\[Y_i \\vert Z_i = 2 \\sim\\mathcal{N}(0, 1.5)\\]\n\nDerive an EM algorithm to estimate the parameters from the siùmulated dataset."
  },
  {
    "objectID": "hmm_lecture.html#baum-welch-algorithm",
    "href": "hmm_lecture.html#baum-welch-algorithm",
    "title": "Introduction to Hidden markov Model",
    "section": "Baum Welch algorithm",
    "text": "Baum Welch algorithm\nForward backward recursion to compute\n\\[E(Z_{ik}\\vert {\\boldsymbol{Y}})\\]\n\\[E(Z_{ik}Z_{j\\ell}\\vert {\\boldsymbol{Y}})\\]"
  },
  {
    "objectID": "hmm_lecture.html#state-decoding-viterbi-algorithm",
    "href": "hmm_lecture.html#state-decoding-viterbi-algorithm",
    "title": "Introduction to Hidden markov Model",
    "section": "State decoding : Viterbi Algorithm",
    "text": "State decoding : Viterbi Algorithm"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hidden Markov model for movement data in Ecology",
    "section": "",
    "text": "Hidden Markov Models are a powerful class of statistical models that are particularly well-suited to analyzing sequential data, where observed patterns are influenced by underlying, unobservable (hidden) states.\nIn this course, we will explore the basic concepts linked with Hidden Markov Models (HMM):\n\nDefinition\nInference with partially observed data - EM algortihm\nBaum Welch Algortihm, EM algorithm in the context of HMM\nViterbi algorithm to reconstruct Hidden states"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Hidden Markov model for movement data in Ecology",
    "section": "",
    "text": "Hidden Markov Models are a powerful class of statistical models that are particularly well-suited to analyzing sequential data, where observed patterns are influenced by underlying, unobservable (hidden) states.\nIn this course, we will explore the basic concepts linked with Hidden Markov Models (HMM):\n\nDefinition\nInference with partially observed data - EM algortihm\nBaum Welch Algortihm, EM algorithm in the context of HMM\nViterbi algorithm to reconstruct Hidden states"
  },
  {
    "objectID": "index.html#the-lab-session",
    "href": "index.html#the-lab-session",
    "title": "Hidden Markov model for movement data in Ecology",
    "section": "The lab session",
    "text": "The lab session\nIn ecological research, HMMs have gained prominence for their ability to extract meaningful behavioral states from movement data, such as distinguishing between foraging, resting, and traveling in animals.\nWe will apply the HMM concepts to analyze movement of animals and we will focus on three Red-footed Boobies (Sula sula). as case studies (trip1, trip2, trip3).\nThe data used in this lab were collected through the work of Sophie Bertrand (IRD), Guilherme Tavares (UFRGS), Christophe Barbraud, and Karine Delord (CNRS) and i would like to thank the International Associated Young Team (JEAI) IRD Tabasco for providing access to these data."
  },
  {
    "objectID": "references_hmm.html",
    "href": "references_hmm.html",
    "title": "References",
    "section": "",
    "text": "liste &lt;- \n  readLines('hmm.bib') |&gt; \n  as_tibble() |&gt; \n  filter(stringr::str_detect(value, pattern = '@|keywords')) |&gt; \n  rowid_to_column() %&gt;% \n  mutate(int_part = floor((rowid-1)/2), parity = rowid %% 2) %&gt;% \n  select(-rowid) %&gt;% \n  pivot_wider( values_from = value, names_from = parity, names_prefix = \"key\") |&gt; \n  mutate(key1 = stringr::str_remove(key1, pattern = '@[:alnum:]+\\\\{')) |&gt; \n  mutate(key1 = stringr::str_remove(key1, pattern = ',')) \n\nWarning in readLines(\"hmm.bib\"): incomplete final line found on 'hmm.bib'\n\nmaf_liste &lt;- liste %&gt;% \n  filter(str_detect(key0, 'MAF')) %&gt;% \n  select(key1) %&gt;% \n  pull()\n\nlinear_liste &lt;- liste %&gt;% \n  filter(!str_detect(key0, 'MAF')) %&gt;% \n  select(key1) %&gt;% \n  pull()\n  \nNoCite(bib = myBib, linear_liste)"
  },
  {
    "objectID": "references_hmm.html#some-useful-references-to-dig-further-in-hmm",
    "href": "references_hmm.html#some-useful-references-to-dig-further-in-hmm",
    "title": "References",
    "section": "Some useful references to dig further in HMM",
    "text": "Some useful references to dig further in HMM\nAkaike, H. (1973). “Information theory as an extension of the maximum likelihood principle. Á In: Petrov, BN and Csaki, F”. In: Second International Symposium on Information Theory. Akademiai Kiado, Budapest, pp. 276Á281.\nBiernacki, C., G. Celeux, and G. Govaert (2000). “Assessing a mixture model for clustering with the integrated completed likelihood”. In: IEEE transactions on pattern analysis and machine intelligence 22.7, pp. 719-725.\nCappe Oliverand Moulines, E. and T. Ryden (2005). Inference in Hidden Markov Models. Springer.\nCarpenter, B., A. Gelman, M. D. Hoffman, et al. (2017). “Stan: A probabilistic programming language”. In: Journal of statistical software 76.1.\nde Valpine, P., D. Turek, C. Paciorek, et al. (2017). “Programming with models: writing statistical algorithms for general model structures with NIMBLE”. In: Journal of Computational and Graphical Statistics 26 (2), pp. 403-413. DOI: 10.1080/10618600.2016.1172487.\nFreitas, C., C. Lydersen, M. A. Fedak, et al. (2008). “A simple new algorithm to filter marine mammal Argos locations”. In: Marine Mammal Science 24.2, pp. 315-325.\nGuédon, Y. (2007). “Exploring the state sequence space for hidden Markov and semi-Markov chains”. In: Computational Statistics & Data Analysis 51.5, pp. 2379-2409.\nJammalamadaka, S. R. and A. Sengupta (2001). Topics in circular statistics. Vol. 5. world scientific.\nLunn, D. J., A. Thomas, N. Best, et al. (2000). “WinBUGS-a Bayesian modelling framework: concepts, structure, and extensibility”. In: Statistics and computing 10.4, pp. 325-337.\nMichelot, T., R. Langrock, and T. A. Patterson (2016). “moveHMM: an R package for the statistical modelling of animal movement data using hidden Markov models”. In: Methods in Ecology and Evolution 7.11, pp. 1308-1315. DOI: 10.1111/2041-210X.12578. eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12578. URL: https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12578.\nMorales, J. M., D. T. Haydon, J. Frair, et al. (2004). “Extracting more out of relocation data: building movement models as mixtures of random walks”. In: Ecology 85.9, pp. 2436-2445.\nPatterson, T. A., L. Thomas, C. Wilcox, et al. (2008). “State–space models of individual animal movement”. In: Trends in Ecology & Evolution 23.2, pp. 87 - 94. ISSN: 0169-5347. DOI: https://doi.org/10.1016/j.tree.2007.10.009. URL: http://www.sciencedirect.com/science/article/pii/S0169534707003588.\nRabiner, L. R. (1989). “A tutorial on hidden Markov models and selected applications in speech recognition”. In: Proceedings of the IEEE 77.2, pp. 257-286."
  }
]